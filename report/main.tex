\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    % \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
    % \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}         % colors


% define colors
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% line style
\lstdefinestyle{mystyle}{
backgroundcolor=\color{backcolour},
commentstyle=\color{codegreen},
keywordstyle=\color{magenta},
numberstyle=\tiny\color{codegray},
stringstyle=\color{codepurple},
basicstyle=\footnotesize\ttfamily,
breakatwhitespace=false,
breaklines=true, captionpos=b,
keepspaces=true, numbers=left,
numbersep=5pt, showspaces=false,
showstringspaces=false,
showtabs=false, tabsize=2,
}
\lstset{style=mystyle}

\makeatletter
\renewcommand{\@noticestring}{}
\makeatother

\title{Statistic Machine Learning Project}



% The \author macro works with any number of authors. There are two commands  
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Zhiwei Y. \thanks{Deploying boosting method to given data and writing \textit{Abstrect, Introduction, and Exploratory data analysis}}
  \textt{zhiwei.yan@angstrom.uu.se}
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
  To fulfill the demand of public bike that more fossil based transportation cans be replaced by bike and contribute to alleviating climate change, the city of Washington D.C. has recorded the observations high bike demand along with temporal and meteorological features. With this data, this project study is dedicated to help the city to predict the necessity of increasing number of bikes at certain hours. To achieve this goal, methods including Logistic Regression, KNN, bagging and boosting are deployed to predict the target label with given feature.  \\ 
  \textit{Number of group members: 3}.
\end{abstract}


\section{Study case and Data} 
To further understand and practice the knowledge we have obtained from lectures. A case of classification problem was given that we can apply different methods on the data and understand the charactoristics and behaviours of these methods.

\subsection{Case and data description}
To help the city of Washington, D.C. understand whether increasing the number of public bikes is neccessary at some certain hour, a machine learning model is expected to predict the public bike demand for given temporal and meteorological features. \\

A data set contains 1600 random obeservations is provided for the model training. The target variable is binary for "high" or "low" demand for increasing bikes' number. The description of the features are given in Table~\ref{tab:feature_description}

\begin{table}[!ht]
  \caption{Labels and features in the data set \citep{smlproject2024}}
  \label{tab:feature_description}
  \centering
    \begin{tabular}{p{4cm} p{8cm}}
    \toprule
    \textbf{Feature Name} & \textbf{Description} \\

    midrule
    increase\_stock \newline (prediction label) &
    \textbf{low\_bike\_demand} – no need to increase the number of bikes \newline
    \textbf{high\_bike\_demand} – the number of bikes needs to be increased \\
    \
    midrule
    hour\_of\_day & Hour of the day (from 0 to 23) \\
    day\_of\_week & Day of the week (from 0 – Monday to 6 – Sunday) \\
    month & Month (from 0 – January to 12 – December) \\
    holiday & If it is a holiday or not (0 – no holiday, 1 – holiday) \\
    weekday & If it is a weekday or not (0 – weekend, 1 – weekday) \\
    summertime & If it is summertime or not (0 – no summertime, 1 – summertime) \\
    temp & Temperature in Celsius degrees \\
    dew & Dew point in Celsius degrees \\
    humidity & Relative humidity (percentage) \\
    precip & Precipitation in mm \\
    snow & Amount of snow in the last hour in mm \\
    snow\_depth & Accumulated amount of snow in mm \\
    windspeed & Wind speed in km/h \\
    cloudcover & Percentage of the city covered in clouds \\
    visibility & Distance in km at which objects or landmarks can be clearly seen and identified \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Exploratory data analysis}
Expolratory data analysis has been conducted to gain the knowledge of relations among features in the dataset. Also, following questions in the project introduction are answered in this section. 

\begin{enumerate}
  \item\label{question:i} Which are the numerical features and which are the categorical features? 
  \item\label{question:ii} Is there any trend to need increase in the availability of bycycles?
\end{enumerate}

Out of that the ways to treat categorical features are different from numerical features, the features are sorted into two groupd, numerical and categorical. This sort of them can be identified according to Table~\ref{tab:feature_description}. The two groups are shown in Table~\ref{tab:cat_num_list}, which also answers Quesiton~\ref{question:i}.

\begin{table}[!ht]
\caption{Categorical and numerical features.}
\label{tab:cat_num_list}
\centering
  \begin{tabular}{p{4cm} p{4cm}}
  \toprule
  \textbf{Categorical features} & \textbf{Numerical features} \\
  \midrule
  holiday         &  month \\
  weekday         &  temp \\
  summertime      & dew \\
  snow            & humidity \\
  increase\_stock & precip \\
  hour\_of\_day   & snow\_depth \\
  day\_of\_week   & windspeed \\
                  & cloudcover \\
  \bottomrule
  \end{tabular}
\end{table}

For categorical features, first step done was to see the label balance. The result is shown in Figure~\ref{fig:cate_dist}. One thing to be noted is that the feature "snow" has only one label. A flat feature will not have input to the model, thus it was excloded from training set. This will be also seen in the correlation analysis. Similarily, histgram plot was generated to visualize the distribution of numerical features (shown in Figure~\ref{fig:num_dist}). 
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.95\linewidth]{figures/count_plot_category.png}
  \caption{Label counts for categorical features}
  \label{fig:cate_dist}
\end{figure}


\begin{figure}[!ht]
  \begin{small}
    \begin{center}
      \includegraphics[width=0.95\textwidth]{figures/num_dist.png}
    \end{center}
    \caption{histgrams of numerical features}
    \label{fig:num_dist}
  \end{small}
\end{figure}

To understand the correlation among features, mostly between targets and other features, correlation analysis has been done ploting corrolation maps, shown in Figure~\ref{fig:correlation}

\begin{figure}[!ht]
  \begin{small}
    \begin{center}
      \includegraphics[width=0.9\textwidth]{figures/corr_map.png}
    \end{center}
    \caption{Corrolation among features}
    \label{fig:correlation}
  \end{small}
\end{figure}

Droping out the smaller correlated features ($correlation \geq 0.1$), the left ones are: "temp", "humidity", "hour\_of\_day", "summertime", "dew", "weekday", and "visibility". To more intuitively see if any features can help seperate the target label. Figure~\ref{fig:box_and_dist} was ploted. 

\begin{figure}[!ht]
  \begin{small}
    \begin{center}
      \includegraphics[width=0.9\textwidth]{figures/box_and_hist_features.png}

    \end{center}
    \caption{Boxs and Histgrams for features with different targets laebls}
    \label{fig:box_and_dist}
  \end{small}
\end{figure}

Finally, with all analysis and figures above, the Question~\ref{question:ii} can be answered. \\ 
It is seen that the label "1" ("high\_bike\_demand") is concerntrated around daytime, growth starts from early morning, drops till late evening after reaching its peak at 15:00 to 16:00. Weekday has more "high\_bike\_demand" than weekends. \\ 
Temperature also has impact on bike demand, more bike are needed when temperture is in a comfortable region, 20 ~ 30 degC in data. This can also interprets the trend in the summertime, temperature is higher in summer, similarily for dew point.

\section{Methodology} 
\label{sec:methodology}
\subsection{Machine learning models}
\subsubsection{Boosting}
Boosting is an emsenble method which is built on the idea that even a week model can capture some patterns between target variables and given features. Starting from a base model, optmizing it based on the returned error can produce a new model which would become the base for next model. By ensembling the predictions from these models, the intention of boosting is to reduce bias\citep{Lindholm2022sml}.\\

In this study, three maintream boosting algorithm were deployed, \textbf{AdaBoost}\citep{Lindholm2022sml}, \textbf{GradientBoost}\citep{Lindholm2022sml}, and \textbf{CatBoost}\citep{prokhorenkova2018catboost}. The methods can expressed as following equations.

\begin{equation}
  \text{AdaBoost:\space \space}
  \hat{y}^{(B)}_{\text{boost}}(\mathbf{x})
  = \operatorname{sign}\!\left(
      \sum_{b=1}^{B} \alpha^{(b)} \hat{y}^{(b)}(\mathbf{x})
    \right)
  \label{eq:adaboost}
\end{equation}


\begin{equation}
  \text{GradientBoost:\space \space}
  f^{(B)}(\mathbf{x}) 
  = \sum_{b=1}^{B} \alpha^{(b)} f^{(b)}(\mathbf{x}),
  \label{eq:gradiantboost}
\end{equation}

\begin{equation}
  \text{CatBoost:\space \space} 
  f^{(B)}(\mathbf{x})
  = \sum_{b=1}^{B} \eta^{(b)} \, T^{(b)}(\mathbf{x})
  \label{eq:catboost}
\end{equation}
\subsubsection{Logistic Regression}
Logistic Regression is one of the most common supervised machine learning models for binary classification problems. Unlike Linear Regression, it uses a sigmoid function, where the raw output value is passed through and converted into a probability between 0 and 1. Since our problem has two possible outcomes, this model is specifically a binomial logistic regression. \\

Logistic Regression first calculates the linear combination of the input features:

\[
z = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_n x_n
\]

Here, \(x_1, x_2, \ldots, x_n\) are the input feature values, \(w_0, w_1, \ldots, w_n\) are the model's learned weights, and \(z\) is the model's internal score. This value is then passed through the sigmoid function to convert it into a probability between 0 and 1.\\

% \textbf{Sigmoid Activation Function}

% The linear output \(z\), which can range from \(-\infty\) to \(+\infty\), is passed through the sigmoid activation function to transform it into a probability \(\mathbb{P}\) between 0 and 1.
  
% \begin{equation}
% \mathbb{P} = \sigma(z) = \frac{1}{1 + e^{-z}}
% \end{equation}

% Here, \(\mathbb{P}\) represents the predicted probability, \(\sigma(z)\) is the sigmoid function which takes the linear combination output \(z\), and \(e^{-z}\) adjusts the curve so that the probability increases as the output \(z\) increases.  

% In the logistic function, classification is made by setting a decision boundary, usually 0.5. If \(\mathbb{P}\) is greater than 0.5, the instance is classified as 1; otherwise, it is classified as 0.
\subsubsection{K-Nearest Neighbours}

K-Nearest Neighbours (KNN) is a supervised machine learning algorithm used for both classification and regression problems. It works by finding the \(K\) nearest points to a given data point and making predictions based on the majority class for classification or the average value for regression.

KNN is a simple, non-parametric, and lazy learning algorithm because it performs computation during prediction rather than during training.

The distance between two data points, \(\mathbf{x}_1\) and \(\mathbf{x}_2\), is measured using the Euclidean distance formula:

\begin{equation}
d(\mathbf{x}_1, \mathbf{x}_2) = \sqrt{(x_{11} - x_{21})^2 + (x_{12} - x_{22})^2 + \cdots + (x_{1n} - x_{2n})^2}
\end{equation}

For a new data point \(\mathbf{X}_{\text{new}}\), the algorithm calculates its distance from all instances in the dataset and selects the \(K\) instances with the smallest distances. The final predicted class is determined based on the majority vote among these \(K\) nearest neighbours.  

It is important to note that if \(K\) is too small, the model becomes overfitted. If \(K\) is too large, the model becomes underfitted. In our project, we selected the optimal \(K\) value using the \texttt{GridSearchCV} function.



\subsection{Validation}
K-fold method with 5 subset was used for cross-validation to the deployed models.
Considering that label are imbalanced in the target variable, metrics like accuracy would not be a good choice for perfomance measurement. F-score, given by equation~\ref{eq:f-score}, is used for measuring the perfomance in this case.  

\begin{equation}
  F_1 = 2 \cdot \frac{precision \cdot recall}{precision + recall} = \frac{2TP}{2TP + FP + FN}
  \label{eq:f-score}
\end{equation}


\section{Results}
% \subsection{Boosting}
% Three boosting models were apply in this study, AdaBoostingClassifier and GradientBoostingClassifier from \textit{Scikit-learn},and CatBoostClassifier. Their perfomance is shown in Table~\ref{tab:models_performance}


% \begin{table}[!ht]
%   \centering
%   \caption{Boosting models performance}
%   \label{tab:models_performance}
%   \begin{tabular}{
%     l
%     cc
%     cc
%     cc
%     c
%   }
%     \toprule

%     \multirow{2}{*}{\textbf{Model}} &
%     \multicolumn{2}{c}{\textbf{Precision}} &
%     \multicolumn{2}{c}{\textbf{Recall}} &
%     \multicolumn{2}{c}{\textbf{F1 Score}} &
%     \textbf{Train Time (s)} \\

%     \cmidrule(lr){2-3}
%     \cmidrule(lr){4-5}
%     \cmidrule(lr){6-7}

%     & \textbf{1} & \textbf{0} & \textbf{1} & \textbf{0} & \textbf{1} & \textbf{0} & \\

%     \midrule
%     AdaBoostClassifier          & 0.6427 & 0.9161 & 0.6298 & 0.9184 & 0.6334 & 0.9170 & 0.0035 \\
%     GradientBoostingClassifier & 0.7729 & 0.9238 & 0.6555 & 0.9549 & 0.7084 & 0.9391 & 0.0890 \\
%     CatBoostClassifier         & 0.7828 & 0.9317 & 0.6932 & 0.9549 & 0.7337 & 0.9431 & 0.5812 \\
%     \bottomrule
    
%   \end{tabular}
% \end{table}



As mentioned in Section~\ref{sec:methodology}, different models of four family were deployed. The result is shown in Table~\ref{tab:best_performance}. 

\begin{table}[!ht]
  \centering
    \caption{f1-score of best models}
    \label{tab:best_performance}
    \begin{tabular}{c
      cc
      cc
      cc
      cc}
    \toprule
      \textbf{model}    & \multicolumn{2}{c}{\textbf{Cat Boost}} & \multicolumn{2}{c}{\textbf{KNN}} & \multicolumn{2}{c}{\textbf{Logistic Regression}} & \multicolumn{2}{c}{\textbf{Random Forest}}\\
      
      \cmidrule{2-3}
      \cmidrule{4-5}
      \cmidrule{6-7}
      \cmidrule{8-9}
  
                        & 0    & 1    & 0    & 1    & 0    & 1    & 0    & 1    \\
      \midrule
      \textbf{F1-score} & 0.93 & 0.61 & 0.92 & 0.57 & 0.94 & 0.62 & \colorbox{yellow}{0.96} & \colorbox{yellow}{0.82} \\ 
      \textbf{Support}  & 270  & 50   & 270  & 50   & 270  & 50   & 262  & 58 \\

    \bottomrule
  \end{tabular}
\end{table}




\section{Discussion}

\section{Conclusion}



% \citep{breiman2001randomforest}   % (Breiman, 2001)
% \citet{hastie2009elements}        % Hastie et al. (2009)



\bibliographystyle{unsrtnat}
\bibliography{references}
\appendix


\section{Appendix}





\end{document}